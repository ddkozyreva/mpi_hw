# Задание №3 - распределенные вычисления с MPI

## Задание

(eigenvector.cpp) Функция eigenvalue вычисляет максимальное по модулю собственное значение матрицы. Для этого используется алгоритм простой итерации: генерируется случайный вектор x, а затем повторяются вычисления

- $ y←Ax $

- $ y ← y / ||y|| $ (нормализация) 

- $ x←y $

После большого числа итераций в x остается только компонента вдоль собственного
вектора матрицы A с максимальным по модулю собственным числом.
Программа запрашивает целое число N, генерирует случайную симметричную матрицу и проводит итерации указанного алгоритма до тех пор, пока Ax не будет слабо отличаться от $x⋅(x^T⋅A⋅x) / (x^T⋅x) $, после чего выводит отношение Рэлея $R = (x^T⋅A⋅x) / (x^T⋅x)$ как приближение к собственному числу.
В приведенной реализации матрица $A$ хранится на процессах распределенно, а векторы $x$ и $e$ хранятся полностью. На каждой итерации процесс вычисляет часть произведения $y = Ax$, а затем полный вектор собирается через MPI_Allgather.
1. Переписать функцию scatter_matrix, чтобы она работала без предположения делимости N на число процессов нацело (использовать MPI_Scatterv / MPI_Gatherv для распределения данных по процессам) (3б.)
2. Рассмотреть вариант решения, в котором вектор y также хранится распределённо на процессах. В этом случае нужно распараллелить вычисление скалярного произведения $(x^T y)$ и максимального отличия $y_i - R x_i$. Сравнить производительность и эффективность параллелизации двух версий программы. (3б.)
3. Получить зависимость эффективности параллелизации от числа процессов в случае, когда все процессы на одном вычислительном узле и на различных вычислительных узлах. (2б.)

## Теория

| Функция | Описание |
| --- | --- |
| MPI_Init() | Инициализация распаралеленной части программы |
| MPI_Finalize() |  Завершение распаралеленной части программы |
| MPI_Wtime() | Функция для того, чтобы засечь время |
| MPI_Comm_rank() | Получение номера процесса |
| MPI_Comm_size() | Получение общего числа запущенных процессов на потоке |
| MPI_Scatter() | Разбивает сообщение на равные части. Тип/число посылаемых=тип/число получаемых элементов. sendcount - число посылаемых каждому процессу элементов, а не общее их количество. Операция обратна по отношению к Gather. |
| MPI_Scatterv() | Векторный вариант MPI_Scatter. Каждому процессу можем посылать различное число элементов |
| MPI_Gather() (не используется) | Каждый процесс отправляет содержимое своего буфера в корневой процесс. Пришедшие сообщений расставляются по порядку. = MPI_Send(…) * n + MPI_Recv(…) |
| MPI_Gatherv() (не используется) | От каждого процесса можем принимать разное число элементов данных. recvcount - массив |
|MPI_Reduce() | Функция MPI_REDUCE объединяет элементы входного буфера каждого процесса в группе, используя заданную операцию, и возвращает объединенное значение в выходной буфер процесса с номером root. |

## Запуск программы

```sh
#!/bin/sh
#SBATCH -J mpi_c
#SBATCH --ntasks-per-node=1     # Количество процессов на узле
#SBATCH -N 1                    # Количество узлов
#SBATCH -A proj_1339
#SBATCH --error=error.txt
#SBATCH --output=output.txt

mpicxx -O3 eigenvector.cpp -o main 
echo 1000 | srun --mpi=pmix ./main
```

_Bash-скрипт запускаю с помощью `sbatch run.sh`._

_При запуске программы иногда меняла параметр --ntasks-per-node на 3, 6, 9 с целью исследовать производительность программы._

## Решение

### Задание №1

_Представлено в файле eigenvector1.cpp._

1. Подготовить всё необходимое для Scatterv().

_Для того, чтобы распараллеливание с разным числом элементов данных стало возможно, воспользуемся MPI_Scatterv(). Ее преимущество перед Scatter() в том, что в разные процессы можно передавать разное число элементов. Потому сначала определим, сколько элементов будет передано в каждый процесс._

>_Основная идея - посмотреть, как размерность матрицы делится на число выделенных процессов и если не нацело, то остаток раскидать по первым k=remain процессам. Вдобавок определить массив сдвигов, который требуется для функции Scatterv()._

_Итак, следующий код был добавлен в main() в файле eigenvector.cpp до создания матрицы А и вызова функции scatter_matrix()._

```cpp
    // Теущий сдвиг
    int current_display = 0;
    // Остаток от размерности матрицы, деленной на количество процессов
    int remain = n % world_size;
    // Целочисленный массив, в i-ячейках которого хранится количество данных, которые пойдут на i-тый процесс
    int *sendcounts = new int[world_size];
    // Целочисленный массив сдвигов относительно начала передаваемых данных
    int *displs = new int[world_size];

    // Рассчитаем массивы sendcounts и displs. Итерируемся по числу процессов
    for (int i = 0; i < world_size; i++) {
        displs[i] = current_display;
        // Количество элементов (чисел), принимаемых i-м процессом 
        sendcounts[i] = int(n / world_size) * n; 
        // Распределим (более-менее) равномерно по процессам остаток от деления
        if (remain > 0) {
           sendcounts[i] += n;  // так как в каждой строке n элементов
           remain--;
        }
        current_display += sendcounts[i];
    }

```

2. Исправить функцию scatter_matrix().

_В функцию scatter_matrix() теперь передаем массивы sendcounts и displs. MPI_Scatter заменили на MPI_Scatterv с необходимыми аргументами._
```cpp
void scatter_matrix(matrix<double> source, matrix<double> dest, int root, int* sendcounts, int* displs, int myrank, MPI_Comm comm)
{
    double *src_ptr = source.raw_ptr(), *dest_ptr = dest.raw_ptr();
    MPI_Scatterv(src_ptr, sendcounts, displs, MPI_DOUBLE, dest_ptr, sendcounts[myrank], MPI_DOUBLE, root, comm);
}
```

3. Отредактировать eigenvalue().

_Собирать процессы будем с помощью MPI_Allgatherv(). Для него рассчитаем все требующиеся аргументы в начале функции eigenvalue()._

```cpp
    MPI_Comm_rank(comm, &myrank);
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    int *scounts = new int[world_size];
    int *displs = new int[world_size];
    int current_display = 0;

    for (int i = 0; i < world_size; i++) {
        scounts[i] = sendcounts[i] / n;
        displs[i] = current_display;
        current_display += scounts[i];
    }
```

_Затем после расчетов соберем все процессы вместе._

```cpp
    MPI_Allgatherv(v1.raw_ptr() + ilocal_start, m, MPI_DOUBLE, v1.raw_ptr(), scounts, displs, MPI_DOUBLE, comm);
```

### Задание №2
_Представлено в файле eigenvector2.cpp._

> В задании использовала MPI_Reduce и MPI_Bcast: после подсчета скалярного произведения и после нахождения ошибки. Первой командой объединяла элементы входного буфера каждого процесса (для скалярного использовала сумму, для ошибки - максимум), а второй рассылала сообщение всем процессам с ссылкой на полученную переменную. Также в сравнении с первым заданием изменила входные данные для eigenvalue() в main().


|  | Параллелизация 1 | Параллелизация 2 |
| --- | --- | --- |
| 1 процесс | 2.39 | 2.47 |
| 3 процесса | 0.87 | 0.87 |
| 6 процессов | 0.42 | 0.43 |
| 9 процессов | 0.30 | 0.34 |

Сравнение производилось на матрице 1000х1000. Ответ во всех экспериментах один: -44.658. Параллелизация в первом задании ни в чем не уступила решению №2, даже чуть-чуть оказалась быстрее.  На кластере результаты сохранены в папке output/.